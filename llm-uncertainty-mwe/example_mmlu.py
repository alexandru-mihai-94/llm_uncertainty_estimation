"""
Example: Using MMLU dataset for evaluation.

This script demonstrates how to:
1. Load the included MMLU dataset
2. Process questions and get predictions
3. Compare with ground truth answers
4. Compute comprehensive metrics
"""

import json
import numpy as np
from uncertainty_estimator import UncertaintyEstimator
from difflib import SequenceMatcher


def load_dataset(filepath='data/mmlu_sample_100.json'):
    """Load dataset from JSON file."""
    with open(filepath, 'r') as f:
        return json.load(f)


def simple_answer_match(llm_answer, true_answer):
    """
    Simple answer matching using substring containment.

    Args:
        llm_answer: Answer generated by LLM
        true_answer: Ground truth answer

    Returns:
        True if match, False otherwise
    """
    llm_lower = llm_answer.lower().strip()
    true_lower = true_answer.lower().strip()

    # Check if true answer is substring of LLM answer
    if true_lower in llm_lower:
        return True

    # Check reverse (sometimes LLM is more concise)
    if llm_lower in true_lower:
        return True

    return False


def fuzzy_answer_match(llm_answer, true_answer, threshold=0.7):
    """
    Fuzzy answer matching using string similarity.

    Args:
        llm_answer: Answer generated by LLM
        true_answer: Ground truth answer
        threshold: Similarity threshold (0-1)

    Returns:
        True if similarity >= threshold, False otherwise
    """
    llm_lower = llm_answer.lower().strip()
    true_lower = true_answer.lower().strip()

    similarity = SequenceMatcher(None, llm_lower, true_lower).ratio()
    return similarity >= threshold


def evaluate_results(results, ground_truth, match_func=simple_answer_match):
    """
    Evaluate results against ground truth.

    Args:
        results: List of result dictionaries from estimator
        ground_truth: List of correct answers
        match_func: Function to match answers

    Returns:
        Dictionary of metrics and detailed results
    """
    assert len(results) == len(ground_truth)

    # Match answers
    matches = []
    for result, true_answer in zip(results, ground_truth):
        is_correct = match_func(result['answer'], true_answer)
        matches.append(is_correct)

    # Extract predictions and confidences
    predictions = [r['predicted_correct'] for r in results]
    confidences = [r['confidence_score'] for r in results]

    # Confusion matrix
    tp = sum(1 for pred, actual in zip(predictions, matches) if pred and actual)
    fp = sum(1 for pred, actual in zip(predictions, matches) if pred and not actual)
    tn = sum(1 for pred, actual in zip(predictions, matches) if not pred and not actual)
    fn = sum(1 for pred, actual in zip(predictions, matches) if not pred and actual)

    # Base metrics
    total = len(matches)
    llm_accuracy = sum(matches) / total if total > 0 else 0

    # Predictor metrics
    predictor_accuracy = (tp + tn) / total if total > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    # Confidence analysis by correctness
    correct_confidences = [conf for conf, match in zip(confidences, matches) if match]
    incorrect_confidences = [conf for conf, match in zip(confidences, matches) if not match]

    return {
        'base_accuracy': llm_accuracy,
        'predictor_accuracy': predictor_accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': {'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn},
        'total_questions': total,
        'correct_answers': sum(matches),
        'incorrect_answers': total - sum(matches),
        'predicted_correct': sum(predictions),
        'predicted_incorrect': total - sum(predictions),
        'mean_confidence_on_correct': np.mean(correct_confidences) if correct_confidences else 0,
        'mean_confidence_on_incorrect': np.mean(incorrect_confidences) if incorrect_confidences else 0,
    }


def main():
    print("="*70)
    print("MMLU Dataset Evaluation Example")
    print("="*70)
    print()

    # Load dataset (use sample for quick testing)
    print("Loading dataset...")
    dataset = load_dataset('data/mmlu_sample_100.json')
    print(f"Loaded {len(dataset)} questions\n")

    # Show sample questions
    print("Sample questions:")
    for i, item in enumerate(dataset[:3], 1):
        q = item['question']
        if len(q) > 80:
            q = q[:77] + "..."
        print(f"{i}. {q}")
        print(f"   Answer: {item['answer']}\n")

    # Initialize estimator
    print("="*70)
    print("Initializing estimator...")
    print("="*70)
    estimator = UncertaintyEstimator(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        token_weight=0.5,
        activation_weight=0.5,
    )

    # Process questions (use subset for demo)
    n_questions = 20  # Process first 20 for demo
    print(f"\nProcessing first {n_questions} questions...")
    print("(This will take a few minutes on first run due to model download)\n")

    questions = [item['question'] for item in dataset[:n_questions]]
    true_answers = [item['answer'] for item in dataset[:n_questions]]

    results = estimator.estimate_batch(
        questions=questions,
        temperature=0.1,
        max_new_tokens=50,
        show_progress=True
    )

    # Evaluate
    print("\n" + "="*70)
    print("EVALUATION RESULTS")
    print("="*70)

    metrics = evaluate_results(results, true_answers, match_func=simple_answer_match)

    print(f"\nLLM Base Performance:")
    print(f"  Accuracy: {metrics['base_accuracy']:.1%}")
    print(f"  Correct: {metrics['correct_answers']}/{metrics['total_questions']}")
    print(f"  Incorrect: {metrics['incorrect_answers']}/{metrics['total_questions']}")

    print(f"\nUncertainty Predictor Performance:")
    print(f"  Predictor Accuracy: {metrics['predictor_accuracy']:.1%}")
    print(f"  Precision: {metrics['precision']:.1%}")
    print(f"  Recall: {metrics['recall']:.1%}")
    print(f"  F1 Score: {metrics['f1']:.1%}")

    print(f"\nPredictions:")
    print(f"  Predicted Correct: {metrics['predicted_correct']}/{metrics['total_questions']}")
    print(f"  Predicted Incorrect: {metrics['predicted_incorrect']}/{metrics['total_questions']}")

    cm = metrics['confusion_matrix']
    print(f"\nConfusion Matrix:")
    print(f"  True Positives (predicted ✓, was ✓): {cm['tp']}")
    print(f"  False Positives (predicted ✓, was ✗): {cm['fp']}")
    print(f"  True Negatives (predicted ✗, was ✗): {cm['tn']}")
    print(f"  False Negatives (predicted ✗, was ✓): {cm['fn']}")

    print(f"\nConfidence Analysis:")
    print(f"  Mean confidence on correct answers: {metrics['mean_confidence_on_correct']:.1%}")
    print(f"  Mean confidence on incorrect answers: {metrics['mean_confidence_on_incorrect']:.1%}")

    # Show detailed examples
    print("\n" + "="*70)
    print("DETAILED EXAMPLES")
    print("="*70)

    for i, (result, true_ans, item) in enumerate(zip(results[:5], true_answers[:5], dataset[:5]), 1):
        is_correct = simple_answer_match(result['answer'], true_ans)

        print(f"\n{i}. {item['question'][:100]}...")
        print(f"   Expected: {true_ans}")
        print(f"   LLM Answer: {result['answer']}")
        print(f"   Match: {'✓ Correct' if is_correct else '✗ Incorrect'}")
        print(f"   Confidence: {result['confidence_score']:.1%}")
        print(f"   Predicted: {'✓ Correct' if result['predicted_correct'] else '✗ Incorrect'}")

    # Analysis notes
    print("\n" + "="*70)
    print("NOTES")
    print("="*70)
    print("""
Based on empirical findings:

1. Small models (<4B params) typically achieve <20% accuracy on MMLU
2. This creates ~85% incorrect vs ~15% correct class imbalance
3. Predictors trained on this data are biased toward predicting "incorrect"
4. Low base accuracy makes it hard to achieve high predictor performance

Recommendations:
- Use larger models (7B+) for better base accuracy
- Consider balanced sampling if training on this data
- Use confidence scores for ranking rather than binary prediction
- Validate on multiple datasets to check generalization

To run on full dataset (10,000 questions):
  dataset = load_dataset('data/mmlu_10k_answers.json')
  # This will take several hours on CPU, ~30-60 min on GPU
    """)


if __name__ == "__main__":
    main()
